{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b61c19-b76d-4eb3-b70c-9c82a8035b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dane zostaly zapisane do pliku 'cennik_kortow_czyste_ascii.csv'\n",
      "Zeskrobane dane o cenach kortow (rowniez wyswietlone w konsoli):\n",
      "{'days': 'Poniedziaek  Piatek', 'time_range': '7:00 - 17:00', 'price_regular': 65.0, 'price_discounted': 55.0}\n",
      "{'days': 'Poniedziaek  Piatek', 'time_range': '17:00 - 23:00', 'price_regular': 95.0, 'price_discounted': 85.0}\n",
      "{'days': 'Weekendy i swieta', 'time_range': '8:00 - 22:00', 'price_regular': 80.0, 'price_discounted': 70.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata # Dodajemy unicodedata do normalizacji znaków\n",
    "\n",
    "def scrape_court_prices(url, csv_filename=\"cennik_kortow_czyste_ascii.csv\"):\n",
    "    \"\"\"\n",
    "    Scrapes court pricing information from the given URL and saves it to a CSV file\n",
    "    with only ASCII characters and numerical prices (without currency or /h).\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "        csv_filename (str): The name of the CSV file to save the data to.\n",
    "                            Defaults to \"cennik_kortow_czyste_ascii.csv\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              'days', 'time_range', 'price_regular', and 'price_discounted'\n",
    "              for a pricing block. Returns an empty list if data cannot be scraped.\n",
    "    \"\"\"\n",
    "    all_court_pricing_data = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        korty_heading_strong = soup.find('strong', string='Korty:')\n",
    "\n",
    "        if korty_heading_strong:\n",
    "            common_parent_of_pricing_blocks = korty_heading_strong.find_parent('div').find_parent('div')\n",
    "\n",
    "            if common_parent_of_pricing_blocks:\n",
    "                # Wyszukujemy TEKST Z ORYGINALNYMI POLSKIMI ZNAKAMI, TAK JAK JEST NA STRONIE.\n",
    "                pricing_blocks = common_parent_of_pricing_blocks.find_all(\n",
    "                    lambda tag: tag.name == 'div' and tag.find('strong', string=re.compile(r'Poniedziałek – Piątek|Weekendy i święta'))\n",
    "                )\n",
    "            else:\n",
    "                print(\"Nie znaleziono wspolnego rodzica dla blokow cenowych.\")\n",
    "                return []\n",
    "        else:\n",
    "            print(\"Nie znaleziono naglowka 'Korty:'.\")\n",
    "            return []\n",
    "\n",
    "        if not pricing_blocks:\n",
    "            print(\"Nie znaleziono blokow cenowych. Sprawdz selektory lub strukture strony.\")\n",
    "            return []\n",
    "\n",
    "        # Funkcja pomocnicza do konwersji tekstu na czyste ASCII i usunięcia niepotrzebnych znaków\n",
    "        def clean_to_ascii(text):\n",
    "            if text is None:\n",
    "                return None\n",
    "            text = str(text)\n",
    "            \n",
    "            # 1. Normalizacja Unicode, by rozłożyć znaki diakrytyczne (np. 'ą' -> 'a' + ogonek)\n",
    "            # a następnie usunąć te znaki diakrytyczne\n",
    "            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "            \n",
    "            # 2. Zamiana specyficznych znaków i symboli\n",
    "            text = text.replace(' – ', ' - ').replace('zł/h', '').replace('/h', '').replace('zł', '') # Myślnik, waluta, /h\n",
    "            text = text.strip() # Usuń spacje na początku i końcu\n",
    "            \n",
    "            # 3. Usuń wszystkie niecyfrowe znaki (oprócz kropki dla liczb dziesiętnych)\n",
    "            # Należy to robić ostrożnie, aby nie usunąć ważnych separatorów\n",
    "            # Dla cen, usuniemy wszystko, co nie jest cyfrą ani kropką/przecinkiem, a potem wyczyścimy\n",
    "            \n",
    "            return text\n",
    "\n",
    "        # Funkcja do wyciągnięcia samej liczby z ceny\n",
    "        def extract_price_number(price_text):\n",
    "            if price_text is None:\n",
    "                return None\n",
    "            \n",
    "            # Usuń wszystkie niecyfrowe znaki oprócz kropek/przecinków\n",
    "            cleaned_price = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            \n",
    "            # Jeśli przecinek jest używany jako separator dziesiętny, zamień na kropkę\n",
    "            if ',' in cleaned_price and '.' not in cleaned_price:\n",
    "                cleaned_price = cleaned_price.replace(',', '.')\n",
    "            \n",
    "            try:\n",
    "                return float(cleaned_price)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "        for block in pricing_blocks:\n",
    "            data = {}\n",
    "\n",
    "            days_strong_tag = block.find('strong')\n",
    "            if days_strong_tag:\n",
    "                data['days'] = clean_to_ascii(days_strong_tag.get_text(strip=True))\n",
    "\n",
    "                time_node = days_strong_tag.next_sibling\n",
    "                if time_node and isinstance(time_node, str):\n",
    "                    time_raw = time_node.replace('\\xa0', ' ').replace('&nbsp;', ' ').strip()\n",
    "                    time_match = re.search(r'(\\d{1,2}:\\d{2} – \\d{1,2}:\\d{2})', time_raw)\n",
    "                    if time_match:\n",
    "                        # W zakresie godzin zamień tylko myślnik na zwykły myślnik ASCII\n",
    "                        data['time_range'] = time_match.group(1).replace(' – ', ' - ')\n",
    "                    else:\n",
    "                        data['time_range'] = None\n",
    "                else:\n",
    "                    data['time_range'] = None\n",
    "\n",
    "            all_strong_tags_in_block = block.find_all('strong')\n",
    "\n",
    "            if len(all_strong_tags_in_block) >= 2:\n",
    "                price_regular_raw = all_strong_tags_in_block[1].get_text(strip=True)\n",
    "                data['price_regular'] = extract_price_number(price_regular_raw)\n",
    "\n",
    "                discount_text_node = all_strong_tags_in_block[1].next_sibling\n",
    "                if discount_text_node and isinstance(discount_text_node, str):\n",
    "                    match = re.search(r'\\(w karnecie\\s*(\\d+[,.]?\\d*\\s*zł/h)\\)', discount_text_node) # Regex szuka \"zł/h\" lub \"zl/h\"\n",
    "                    if match:\n",
    "                        price_discounted_raw = match.group(1)\n",
    "                        data['price_discounted'] = extract_price_number(price_discounted_raw)\n",
    "                    else:\n",
    "                        data['price_discounted'] = None\n",
    "                else:\n",
    "                    data['price_discounted'] = None\n",
    "            else:\n",
    "                data['price_regular'] = None\n",
    "                data['price_discounted'] = None\n",
    "\n",
    "            if data:\n",
    "                all_court_pricing_data.append(data)\n",
    "\n",
    "        df = pd.DataFrame(all_court_pricing_data)\n",
    "        \n",
    "        # Zapis do pliku CSV z kodowaniem UTF-8.\n",
    "        # Dane w DataFrame są już oczyszczone do ASCII/liczb.\n",
    "        # To powinno rozwiązać problem z \"krzakami\" w programie otwierającym CSV.\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\nDane zostaly zapisane do pliku '{csv_filename}'\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Blad podczas pobierania strony: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wystapil nieoczekiwany blad: {e}\")\n",
    "\n",
    "    return all_court_pricing_data\n",
    "\n",
    "# URL strony do zeskrobania\n",
    "url = \"https://www.kortypraga.pl/cennik-2-2/\"\n",
    "\n",
    "# Wywolanie funkcji\n",
    "pricing_info = scrape_court_prices(url)\n",
    "\n",
    "if pricing_info:\n",
    "    print(\"Zeskrobane dane o cenach kortow (rowniez wyswietlone w konsoli):\")\n",
    "    for item in pricing_info:\n",
    "        print(item)\n",
    "else:\n",
    "    print(\"Nie udalo sie zeskrobac danych o cenach kortow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
